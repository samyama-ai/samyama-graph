usecase_id,workflow_id,persona_id,name,description,preconditions,steps,expected_outcome,postconditions,acceptance_criteria,requirements_refs,priority
UC001,WF002,P002,Create Social Network Graph,"Developer creates a social network graph with users and friendship relationships","Database running, Redis client installed, Developer has credentials","1. Connect to Samyama via Redis protocol; 2. Execute CREATE query to add User nodes; 3. Execute CREATE query to add FRIENDS_WITH edges; 4. Query to retrieve friend recommendations","Graph contains User nodes and FRIENDS_WITH relationships, Queries return expected results","Graph schema created, Sample data loaded, Queries validated","Query executes in < 10ms, Correct results returned, No errors",REQ-GRAPH-001|REQ-GRAPH-002|REQ-GRAPH-003|REQ-CYPHER-003|REQ-REDIS-002,High
UC002,WF003,P005,Provision New Tenant with Resource Quotas,"Tenant admin creates isolated namespace for a new customer with 4GB memory limit","Cluster running, Admin credentials available, Customer info ready","1. Call management API to create tenant 'customer_A'; 2. Set memory quota to 4GB; 3. Set storage quota to 100GB; 4. Set max connections to 50; 5. Create tenant admin user; 6. Verify tenant isolation","Tenant 'customer_A' created with quotas, Tenant admin can authenticate, Resource quotas enforced","Tenant operational, Quotas configured, Monitoring active","Tenant isolated from others, Memory usage cannot exceed 4GB, Admin user can authenticate",REQ-TENANT-001|REQ-TENANT-002|REQ-TENANT-003|REQ-TENANT-004|REQ-TENANT-007,Critical
UC003,WF005,P003,Execute Multi-Hop Traversal on Billion-Node Graph,"Data scientist runs 3-hop friend-of-friend query on 1B node social graph","1B node graph loaded, Indices created, Cluster scaled to handle load","1. Write Cypher query: MATCH (a:Person)-[:KNOWS*1..3]->(b:Person) WHERE a.id = 123 RETURN b; 2. Execute query; 3. Measure execution time; 4. Verify result count","Query completes in < 100ms, Results accurate, Memory usage acceptable","Query executed successfully, Results cached for reuse, Performance logged","Execution time < 100ms, Correct results returned, Memory usage < 80% of available",REQ-PERF-005|REQ-CYPHER-005|REQ-CYPHER-006|REQ-PERF-007,High
UC004,WF004,P001,Automatic Failover After Leader Node Crash,"DBA validates that cluster automatically elects new leader when current leader crashes","3-node Raft cluster running, All nodes healthy, Test writes in progress","1. Identify current leader node; 2. Forcefully kill leader process; 3. Observe leader election process; 4. Measure failover time; 5. Verify new leader elected; 6. Confirm writes resume; 7. Validate data consistency","New leader elected within 30s, Write operations resume, No data loss, All nodes eventually consistent","Cluster recovered, New leader operational, Old leader can rejoin when restarted","Failover time < 30s, No committed writes lost, New leader functional, Cluster operational with 2/3 nodes",REQ-DIST-004|REQ-AVAIL-002|REQ-AVAIL-003|REQ-DIST-006,Critical
UC005,WF006,P001,Backup and Restore Production Database,"DBA performs full backup to S3 and restores to new cluster","Production cluster healthy, S3 bucket configured, Target cluster provisioned for restore","1. Trigger backup via management API; 2. Monitor backup progress; 3. Verify backup file in S3; 4. Checksum validation; 5. Restore to target cluster; 6. Compare node/edge counts; 7. Run sample queries; 8. Validate query results match source","Backup completed successfully, Restore successful, Data integrity verified, Target cluster operational","Backup file in S3, Target cluster contains same data as source, Queries return same results","Backup completes in < 2 hours, Restore successful, Data integrity 100%, No data loss",REQ-OPS-005|REQ-OPS-007|REQ-OPS-008|REQ-PERSIST-007,Critical
UC006,WF008,P002,Optimize Slow Cypher Query with Indices,"Developer optimizes a slow pattern matching query using indices","Query identified as slow (> 100ms), Database access available, EXPLAIN privilege granted","1. Run EXPLAIN on slow query; 2. Identify full table scans; 3. Analyze missing indices on labels/properties; 4. Create index on Person.age; 5. Re-run EXPLAIN; 6. Confirm index usage; 7. Measure new execution time","Query optimized to use index, Execution time reduced from 120ms to 8ms, Query plan improved","Index created, Query performance improved, Query added to application","Execution time < 10ms after optimization, Index used in query plan, Application performance improved",REQ-CYPHER-009|REQ-OPS-003|REQ-PERF-001,High
UC007,WF001,P004,Deploy 3-Node Kubernetes Cluster,"DevOps engineer deploys HA cluster on Kubernetes using StatefulSet","Kubernetes cluster available, kubectl configured, Helm charts available, Storage provisioned","1. Install Helm chart with replication factor 3; 2. Verify pods start successfully; 3. Check persistent volume claims; 4. Verify Raft cluster formation; 5. Create load balancer service; 6. Test connectivity; 7. Load initial data; 8. Configure monitoring","3 Samyama pods running, Raft cluster formed, Load balancer operational, Monitoring active","Cluster operational, Health checks passing, External access configured, Grafana dashboard showing metrics","All pods in Running state, Raft leader elected, Service accessible via load balancer, Prometheus scraping metrics",REQ-COMPAT-003|REQ-DIST-001|REQ-DIST-006|REQ-OPS-002,High
UC008,WF009,P008,Enable TLS and RBAC for Production,"Security engineer enables encryption and access control before production launch","Cluster deployed, TLS certificates obtained, User roles defined, Audit requirements documented","1. Generate TLS certificates for nodes; 2. Configure Samyama to use TLS; 3. Restart nodes with TLS enabled; 4. Create role definitions (admin, read-write, read-only); 5. Assign users to roles; 6. Test access with different roles; 7. Enable audit logging; 8. Verify audit log entries","TLS enabled on all connections, RBAC configured, Users have appropriate permissions, Audit log capturing events","All connections encrypted, Role-based access enforced, Audit logging active, Security scan passed","All connections use TLS, Users can only perform authorized operations, Audit log contains all access events, Penetration test passed",REQ-SEC-001|REQ-SEC-002|REQ-SEC-003|REQ-SEC-005,Critical
UC009,WF010,P004,Detect and Respond to Memory Exhaustion Alert,"DevOps engineer receives alert about high memory usage and scales cluster","Prometheus/Grafana configured, Alert rules defined, On-call engineer notified, Cluster has capacity to scale","1. Receive PagerDuty alert: Memory usage > 85%; 2. Check Grafana dashboard; 3. Identify tenant consuming most memory; 4. Review tenant quotas; 5. Decide to scale cluster; 6. Add new node via management API; 7. Monitor rebalancing; 8. Verify memory usage drops below threshold; 9. Update capacity plan","Alert received, Root cause identified, Cluster scaled, Memory usage normalized, Incident documented","Memory usage < 80%, New node operational, Capacity increased, Post-mortem completed","Alert received within 2 minutes of threshold breach, Issue mitigated within 30 minutes, No service disruption, Root cause documented",REQ-OPS-001|REQ-OPS-002|REQ-AVAIL-005|REQ-MEM-004,High
UC010,WF011,P009,Run Load Test with 10K Concurrent Queries,"QA engineer validates system can handle 10,000 concurrent read queries","Test cluster deployed, Load testing tool configured (e.g., redis-benchmark), Test dataset loaded","1. Configure load test: 10K connections, 60s duration, read queries; 2. Start load test; 3. Monitor query latency (p50, p95, p99); 4. Monitor system resources (CPU, memory, network); 5. Capture metrics; 6. Analyze results; 7. Compare against SLA (< 10ms p99)","Load test completes successfully, p99 latency < 10ms, No errors, System stable under load","Performance validated, Results documented, Any issues identified for fixing","Throughput > 100K queries/sec, p99 latency < 10ms, Error rate 0%, System remains stable",REQ-PERF-002|REQ-SCALE-003|REQ-PERF-001,High
UC011,WF012,P010,Build Recommendation Engine MVP,"Startup CTO builds basic recommendation system using collaborative filtering on graph","Docker installed, Development environment ready, Sample user-item interaction data available","1. Deploy Samyama locally via Docker; 2. Model users and items as nodes; 3. Create PURCHASED/VIEWED edges; 4. Write Cypher query for collaborative filtering (users who bought X also bought Y); 5. Build REST API wrapper; 6. Test with sample data; 7. Deploy to cloud; 8. Onboard beta users","Recommendation API functional, Query latency acceptable, Beta users can get recommendations, Infrastructure cost minimal","MVP deployed, Users getting recommendations, System monitored, Feedback collected","MVP deployed < 1 month, Query latency < 50ms, 10+ beta users onboarded, Cost < $500/month",REQ-REDIS-006|REQ-COMPAT-001|REQ-COMPAT-004,Medium
UC012,WF013,P001,Rolling Upgrade from v1.0 to v1.1,"DBA performs zero-downtime rolling upgrade across 3-node cluster","v1.1 release notes reviewed, Staging upgrade successful, Production backup completed, Maintenance communication sent","1. Drain traffic from node-1; 2. Stop node-1; 3. Upgrade node-1 to v1.1; 4. Start node-1; 5. Verify node-1 rejoins cluster; 6. Repeat for node-2 and node-3; 7. Verify all nodes on v1.1; 8. Monitor for 24 hours; 9. Validate new features","All nodes upgraded to v1.1, No downtime experienced, New features available, System stable","Cluster fully upgraded, Feature flags enabled, Performance baseline re-established, Documentation updated","Zero downtime, All nodes on v1.1, No errors during upgrade, Performance maintained",REQ-AVAIL-004|REQ-OPS-011,High
UC013,WF014,P002,Import CSV Data into Graph,"Developer imports CSV files containing nodes and edges into Samyama","CSV files prepared (users.csv, friendships.csv), Data transformation logic ready, Database access configured","1. Write Python script using redis-py; 2. Use pipelining for batch inserts; 3. Read users.csv and create Person nodes; 4. Read friendships.csv and create FRIENDS_WITH edges; 5. Monitor import progress; 6. Validate record counts; 7. Create indices after import; 8. Run sample queries","Data imported successfully, Record counts match CSV files, Indices created, Queries performant","Graph populated with data, Indices operational, Application can query data, Import script saved for reuse","Import completes in < 1 hour for 1M records, All records imported successfully, Queries use indices, No data corruption",REQ-CYPHER-003|REQ-REDIS-007|REQ-PERF-003,Medium
UC014,WF002,P003,Analyze Knowledge Graph with Path Queries,"Data scientist finds shortest paths between entities in knowledge graph","Knowledge graph loaded (entities as nodes, relationships as edges), Python client installed, Jupyter notebook ready","1. Connect to Samyama from Jupyter; 2. Write query to find shortest path between two entities; 3. Execute query using variable-length patterns; 4. Visualize path results; 5. Run multiple path queries for analysis; 6. Export results to pandas DataFrame; 7. Perform statistical analysis","Shortest paths found, Analysis complete, Results exported to DataFrame, Insights documented","Path analysis complete, Findings documented in notebook, Results reproducible, Code committed to repo","Query finds correct shortest path, Execution time < 50ms, Results exported successfully, Analysis reproducible",REQ-CYPHER-005|REQ-CYPHER-006|REQ-COMPAT-001,Medium
UC015,WF003,P005,Enforce Tenant Memory Quota,"Tenant admin validates that tenant cannot exceed allocated 4GB memory quota","Tenant created with 4GB memory quota, Monitoring configured, Test data generator ready","1. Start loading data into tenant namespace; 2. Monitor memory usage via metrics API; 3. Continue loading until quota approached; 4. Observe quota enforcement (writes rejected or eviction triggered); 5. Verify error message indicates quota exceeded; 6. Validate other tenants unaffected","Memory quota enforced at 4GB, Writes rejected when quota exceeded, Error message clear, Other tenants unaffected","Quota working as designed, Tenant cannot exceed limit, Monitoring alert triggered, Documentation updated","Memory usage cannot exceed 4GB, Clear error message returned, Other tenants operational, No cluster impact",REQ-TENANT-003|REQ-MEM-006|REQ-TENANT-005,Critical
UC016,WF004,P001,Validate Data Consistency After Network Partition,"DBA tests that cluster maintains consistency during and after network partition","3-node cluster running, Network partition simulation tool available, Write load generator ready","1. Start continuous writes to cluster; 2. Simulate network partition isolating 1 node; 3. Observe that minority partition (1 node) rejects writes; 4. Majority partition (2 nodes) continues accepting writes; 5. Heal partition; 6. Verify isolated node catches up via log replication; 7. Validate all nodes have same data","Minority partition rejected writes during partition, Majority partition accepted writes, All nodes consistent after healing, No data loss","Cluster fully recovered, Data consistent across all nodes, Split-brain prevented, Test documented","Writes rejected on minority partition, Writes accepted on majority, No data loss, Consistency validated with checksums",REQ-DIST-005|REQ-AVAIL-003|REQ-DIST-006,Critical
UC017,WF007,P004,Add Nodes to Scale Read Capacity,"DevOps engineer adds 2 read replicas to handle increased query load","Current cluster at capacity (CPU > 80%), New nodes provisioned, Load balancer configured","1. Monitor current read QPS and latency; 2. Add 2 new follower nodes to cluster; 3. Verify nodes join Raft cluster as followers; 4. Configure load balancer to include new nodes for reads; 5. Monitor query distribution; 6. Measure new read capacity; 7. Validate latency improvement","2 new nodes added, Read capacity increased, Latency reduced, Load balanced across nodes","Read QPS doubled, Latency improved, All nodes healthy, Capacity plan updated","Read QPS increased by 2x, p99 latency reduced from 25ms to 10ms, All nodes receiving traffic, No errors",REQ-DIST-003|REQ-SCALE-001|REQ-PERF-002,High
UC018,WF008,P002,Use Aggregation Functions in Query,"Developer writes query to count friends and calculate average age by city","Graph contains Person nodes with age and city properties, FRIENDS_WITH edges exist, Database access configured","1. Write Cypher query with GROUP BY on city; 2. Use COUNT() to count friends; 3. Use AVG() to calculate average age; 4. Add ORDER BY to sort results; 5. Execute query; 6. Validate results; 7. Optimize if needed","Query returns correct aggregated results, Results sorted by city, Execution time acceptable","Query validated and added to application, Results match expected values, Performance acceptable","Correct aggregate values returned, Results properly grouped by city, Execution time < 50ms, Query optimized",REQ-CYPHER-004|REQ-CYPHER-008,Medium
UC019,WF009,P008,Audit Security Events for Compliance,"Security engineer validates audit logs capture all required security events for SOC2 audit","Audit logging enabled, Sample security events to test, Audit log retention configured, SOC2 requirements documented","1. Perform test login (success and failure); 2. Execute privileged operations (create user, delete data); 3. Attempt unauthorized access; 4. Review audit logs; 5. Verify all events captured; 6. Validate log format and content; 7. Test log retention policy; 8. Export logs for auditor review","All security events logged, Log format meets requirements, Retention policy working, Logs exportable for audit","Audit logging validated, Logs ready for compliance audit, Security controls verified, Documentation complete","All authentication attempts logged, Privileged operations captured, Unauthorized access attempts logged, Logs tamper-proof",REQ-SEC-005|REQ-OPS-004,Critical
UC020,WF010,P009,Chaos Testing - Simulate Node Crash During Write,"QA engineer validates system behavior when node crashes during active writes","Test cluster deployed, Chaos engineering tool configured, Write load generator ready, Data integrity validation script prepared","1. Start continuous write operations; 2. Monitor write success rate; 3. Randomly kill one node process; 4. Continue writes; 5. Observe failover behavior; 6. Verify writes continue to succeed; 7. Validate no duplicate or lost writes; 8. Restart killed node; 9. Verify node rejoins and catches up","Node crash doesn't cause write failures after failover, No data lost or duplicated, Crashed node successfully rejoins","System survived node crash, Data integrity maintained, Chaos test documented, Any bugs identified","Write success rate remains high (> 99%), No committed writes lost, No duplicate writes, Crashed node rejoins automatically",REQ-AVAIL-002|REQ-AVAIL-003|REQ-DIST-004,High
UC021,WF002,P002,Connect from Python Application,"Developer connects Python Flask app to Samyama and executes first query","Python and Flask installed, redis-py library installed, Samyama instance running and accessible","1. Import redis library in Python; 2. Create connection to Samyama; 3. Authenticate with credentials; 4. Execute simple GRAPH.QUERY command; 5. Parse response; 6. Handle errors; 7. Close connection; 8. Integrate into Flask route","Python app successfully connects, Query executes and returns results, Error handling works, Results rendered in Flask app","Python integration working, Sample app deployed locally, Developer documentation updated, Code committed","Connection successful, Query returns expected results, Error handling validates, Response time < 100ms",REQ-REDIS-002|REQ-REDIS-006|REQ-COMPAT-001,High
UC022,WF005,P003,Calculate PageRank on Large Graph,"Data scientist runs custom PageRank-like algorithm using Cypher queries","Large graph loaded, Algorithm designed, Python client configured, Computational resources allocated","1. Implement iterative PageRank in Python using Cypher queries; 2. Initialize all nodes with rank 1.0; 3. Iterate: calculate new ranks based on neighbors; 4. Update node properties with new ranks; 5. Repeat until convergence; 6. Extract top-ranked nodes; 7. Validate results; 8. Export to file","PageRank algorithm completes successfully, Results converge, Top nodes identified, Results validated","Algorithm documented, Results exported, Performance acceptable for graph size, Code reusable","Algorithm converges in < 20 iterations, Execution time < 10 minutes for 10M nodes, Results accurate, Code reproducible",REQ-CYPHER-001|REQ-CYPHER-003|REQ-PERF-005,Medium
UC023,WF001,P004,Configure Prometheus Monitoring,"DevOps engineer sets up Prometheus to scrape Samyama metrics","Prometheus server deployed, Samyama metrics endpoint exposed, Grafana available, Alert manager configured","1. Configure Prometheus scrape config for Samyama; 2. Verify Prometheus scraping metrics; 3. Import Samyama Grafana dashboard; 4. Configure alert rules (high latency, node down, etc.); 5. Test alerts by triggering conditions; 6. Verify alerts fire correctly; 7. Document monitoring setup","Prometheus scraping metrics, Grafana dashboard showing data, Alerts configured and tested, Documentation complete","Monitoring operational, Dashboards accessible, Alerts firing correctly, On-call team notified of setup","Prometheus targets all healthy, Metrics visible in Grafana, Test alerts fire within 2 minutes, No scrape errors",REQ-OPS-001|REQ-OPS-002|REQ-AVAIL-005,High
UC024,WF006,P001,Test Point-in-Time Recovery,"DBA validates ability to recover database to specific timestamp","Production database with continuous writes, Backups configured, Point-in-time recovery feature enabled, Test restoration target identified","1. Note current timestamp; 2. Perform some operations (creates, updates, deletes); 3. Note restoration target timestamp (before some operations); 4. Continue operations after target; 5. Initiate point-in-time recovery to target timestamp; 6. Validate restored data matches state at target time; 7. Verify operations after target are not present","Database restored to exact point in time, Data matches expected state at target timestamp, Operations after target not present","Recovery successful, Data validated, Recovery process documented, RTO measured","Restored data matches snapshot at target timestamp, No data corruption, Recovery time < 2 hours, Process documented",REQ-OPS-006|REQ-PERSIST-002|REQ-PERSIST-004,High
UC025,WF015,P006,Design Multi-Region Architecture,"Solutions architect designs globally distributed deployment across 3 AWS regions","Requirements gathered (latency SLAs, data residency, disaster recovery), AWS account configured, Budget approved","1. Design cluster topology (5 nodes across 3 regions); 2. Plan network architecture (VPC peering, private links); 3. Calculate cross-region replication latency; 4. Design failover strategy; 5. Plan data residency compliance; 6. Estimate costs; 7. Create architecture diagram; 8. Document trade-offs; 9. Present to stakeholders; 10. Get approval","Architecture designed and documented, Costs estimated, Trade-offs understood, Stakeholder approval obtained","Architecture approved, Deployment plan ready, Costs within budget, Documentation complete","Architecture supports < 100ms cross-region latency, Disaster recovery plan meets RTO/RPO, Costs within budget, Stakeholders approve",REQ-COMPAT-002|REQ-AVAIL-001|REQ-DIST-001|REQ-SCALE-001,Medium
UC026,WF016,P002,Configure Auto-Embed for Documentation,"Developer configures automatic embedding generation for document nodes","Samyama instance running, LLM provider API key available","1. Create tenant config with OpenAI provider; 2. Define embedding policy: Document nodes, content property; 3. Create Document node with text content; 4. Wait for background processing; 5. Verify vector embedding created","Vector embedding automatically generated and indexed","Auto-Embed configured, Embeddings generated automatically","Embedding generated without manual intervention, Vector search finds the document",REQ-RAG-001|REQ-RAG-002|REQ-RAG-003,High
UC027,WF017,P002,Query Graph using English,"Developer asks questions in plain English to explore data","NLQ enabled for tenant, Graph contains data","1. Send query: \"Who are Alice friends?\"; 2. System generates: MATCH (a:Person {name: \"Alice\"})-[:KNOWS]->(b) RETURN b; 3. System executes query; 4. Returns friend nodes","Correct nodes returned, Generated Cypher shown","User has answer without writing code","Result matches Cypher equivalent, Generated Cypher is valid, No data modified",REQ-NLQ-001|REQ-NLQ-002,High
